{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#paratools-pro-for-e4stm","title":"ParaTools Pro for E4S\u2122","text":"<p>The Extreme-scale Scientific Software Stack (E4S\u2122) is a broad collection of HPC focused software packages. E4S provides a unified computing environment for deployment of open-source projects. E4S includes contributions from many organizations, including national laboratories, universities, and industry. E4S packages are deployed and managed via Spack. E4S was originally developed to provide a common software environment for the exascale leadership computing systems currently being deployed at DOE National Laboratories across the U.S. </p> <p>ParaTools Pro for E4S\u2122 takes E4S and deploys it to virtual machines and containers that are hardened and optimized for use on commercial clouds. It adds additional valuable features such as enhanced MPI performance, a performant remote desktop interface, and extra, optimized software packages for a variety of AI and other HPC applications. ParaTools Pro for E4S\u2122 also adds deployment and development support from ParaTools, Inc. </p> <p>ParaTools Pro for E4S\u2122 is supported by the U.S. Department of Energy's SBIR program.</p>"},{"location":"AWS/getting-started-AWS/","title":"ParaTools Pro for E4S\u2122 Getting Started with Amazon Web Services (AWS)","text":""},{"location":"AWS/getting-started-AWS/#general-background-information","title":"General Background Information","text":"<p>In this tutorial we will show you how to launch an HPC cluster on AWS. You will use the command line tools, AWS CLI, and AWS ParallelCluster to create a .yaml file that describes your head-node, and the cluster-nodes. It will then launch a head-node that can spawn EC2 instances that are linked with EFA networking capabilities.</p> <p>For the purposes of this tutorial, we make the following assumptions: - You have created an AWS account, and an Administrative User</p>"},{"location":"AWS/getting-started-AWS/#tutorial","title":"Tutorial","text":""},{"location":"AWS/getting-started-AWS/#install-aws-parallelcluster","title":"Install AWS ParallelCluster","text":"<p>To install Pcluster, upgrade pip, and install virtualenv if not installed. Note amazon recommends installing pcluster in a virtual environment.  For this section we essentially follow \"Setting Up AWS ParallelCluster\", if you have any issues look there.</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install --user --upgrade virtualenv\n</code></pre> <p>Then create and source the virtual environment: <pre><code>python3 -m virtualenv ~/apc-ve\nsource ~/apc-ve/bin/activate\n</code></pre></p> <p>Then install ParallelCluster. If the version of ParallelCluster does not match the version used to generate the AMI then the cluster creation operation will fail. As of this writing ParaTools Pro for E4S\u2122 AMIs are built with ParallelCluster 3.10.0. Check the version string of your selected ParaTools Pro for E4S\u2122 AMI, visible on the AWS Marketplace listing, for the associated ParallelCluster version. <pre><code>python3 -m pip install --upgrade \"aws-parallelcluster\"==3.10.0\n</code></pre></p> <p>ParallelCluster needs node.js for CloudFormation, so <pre><code>curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash\nchmod ug+x ~/.nvm/nvm.sh\nsource ~/.nvm/nvm.sh\nnvm install --lts\nnode --version\n</code></pre></p>"},{"location":"AWS/getting-started-AWS/#install-aws-command-line-interface","title":"Install AWS Command Line Interface","text":"<p>Now we must install AWS CLI, which will handle authenticating your information every time you create a cluster. For this section we follow \"Installing AWS CLI\", if you have any issues look there.  <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre> Note, if you do not have sudo user rights, you must select the install and bin, with the flags <code>-i</code> and <code>-b</code>, as shown below <pre><code>./aws/install -i ~.local/aws-cli -b ~/.local/bin\n</code></pre></p>"},{"location":"AWS/getting-started-AWS/#aws-security-credentials-and-cli-configuration","title":"AWS Security Credentials and CLI Configuration","text":"<p>For this section we follow Creating Access Keys and Configuring AWS CLI, if you have any issues look there.  If you do not already have a secure access key, you must create one. From the IAM page, on the left side of the page select Users, then select the user you would like to grant access credentials to, then select the Security credentials, and scroll down to Create access key. Create a key for CLI activities. Make sure to save these very securely. </p> <p>Now we can configure AWS with those security credentials. <pre><code>aws configure\n</code></pre> And then enter the respective information, <pre><code>AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nDefault region name [us-east-1]: us-west-2\nDefault output format [None]: json\n</code></pre></p>"},{"location":"AWS/getting-started-AWS/#aws-ec2-key-pair","title":"AWS EC2 Key Pair","text":"<p>To perform cluster tasks, such as running and monitoring jobs, or managing users, you must be able to access the cluster head node. To verify you can access the head node instance using SSH, you must use an EC2 key pair. If you do not already have a key pair you in the region you would like to use, follow this guide to quickly make a key </p>"},{"location":"AWS/getting-started-AWS/#aws-user-policies","title":"AWS user policies","text":"<p>To create and manage clusters in an AWS account, AWS ParallelCluster requires permissions at two levels: * Permissions that the pcluster user requires to invoke the pcluster CLI commands for creating and managing clusters. * Permissions that the cluster resources require to perform cluster actions. The policies described here are supersets of the required permissions to create clusters. If you know what you are doing you can remove permissions as you feel fit. To make the policies, open the IAM page, select Policies on the left, and Create Policy, then select the JSON editor. Copy and paste the policy found here. Unless you plan to use AWS secrets, you must remove the final section from the JSON. <pre><code>      {\n          \"Action\": \"secretsmanager:DescribeSecret\",\n          \"Resource\": \"arn:aws:secretsmanager:&lt;REGION&gt;:&lt;AWS ACCOUNT ID&gt;:secret:&lt;SECRET NAME&gt;\",\n          \"Effect\": \"Allow\"\n      }\n</code></pre> If it reports errors, replace the  with your 12 digit account ID. Then create and name the policy \"ClusterPolicy1\". Create another policy, with this JSON, naming it \"ClusterPolicy2\", similarly replacing account id where it prompts you to. From the policies menu, find and open ClusterPolicy1 and click Entities attached,  and attach the users you would like to be able to create clusters. Repeat this process for \"ClusterPolicy2\". Similarly, in the policies list, find the policy \"AmazonVPCFullAccess\" and attach the users to this. This will allow them to create VPC's if necessary. We have now granted the required permissions to users to create clusters."},{"location":"AWS/getting-started-AWS/#find-the-ami","title":"Find the AMI","text":"<p>You will need to have the AMI (Amazon Machine Image) ready for this next step. Select the ParaTools Pro for E4S\u2122 marketplace listing for the image you want, click subscribe, then continue to configuration, select the correct region, and then copy the AMI Id that is provided. </p>"},{"location":"AWS/getting-started-AWS/#cluster-configuration-and-creation","title":"Cluster configuration and creation","text":"<p>When creating a cluster you will be prompted for: - Region: Select whichever region you are planning to launch these in. - EC2 key pair: Select the one you just created, or plan on using to access the nodes. - Scheduler: You must select slurm - OS: Ubuntu 22.04 -  Head node instance type: As it only controls the nodes it does not require much compute capabilities. A t3.large will often suffice. Note the head node does not have to be EFA capable. -  Structure of your queue should be selected as required by your use case. -  Compute instance types: You must select an EFA capable node. You can find these out by:   <pre><code>aws ec2 describe-instance-types --filters \"Name=processor-info.supported-architecture,Values=x86_64*\" \"Name=network-info.efa-supported,Values=true\" --query   InstanceTypes[].InstanceType\n</code></pre>   Furthermore you can find which EFA capable nodes that have GPU support by   <pre><code>aws ec2 describe-instance-types --filters \"Name=processor-info.supported-architecture,Values=x86_64\" \"Name=network-info.efa-supported,Values=true\" --query   'InstanceTypes[?GpuInfo.Gpus!=null].InstanceType'\n</code></pre> - For the network settings, select as required for your workflow, or follow the given options. - Automatic VPC: Unless you already have a VPC you plan on using, select yes. Be aware that after creating many VPC's you might run into a limit, requiring you to delete older unused ones.</p> <p>To create the cluster-config.yaml file, <pre><code>     `pcluster configure --config cluster-config.yaml`\n</code></pre></p> <pre><code>INFO: Configuration file cluster-config.yaml will be written.\nPress CTRL-C to interrupt the procedure.\n\nAllowed values for AWS Region ID:\n1. ap-northeast-1\n2. ap-northeast-2\n...\n15. us-west-1\n16. us-west-2\nAWS Region ID [us-west-2]:\nAllowed values for EC2 Key Pair Name:\n1. Your-EC2-key\n\nEC2 Key Pair Name [Your-EC2-key]: 1\nAllowed values for Scheduler:\n1. slurm\n2. awsbatch\nScheduler [slurm]: 1\nAllowed values for Operating System: \n1. alinux2\n2. centos7\n3. ubuntu2004\n4. ubuntu2204\nOperating System [ubuntu2204]:\nHead node instance type [t3.large]:\nNumber of queues [1]:\nName of queue 1 [queue1]:\nNumber of compute resources for queue1 [1]:\nCompute instance type for compute resource 1 in queue1 [t3.micro]: t3.micro\nMaximum instance count [10]:\nAutomate VPC creation? (y/n) [n]: y\nAllowed values for Availability Zone:\n1. us-west-2a\n2. us-west-2b\n3. us-west-2c\nAvailability Zone [us-west-2a]: 1\nAllowed values for Network Configuration:\n1. Head node in a public subnet and compute fleet in a private subnet\n2. Head node and compute fleet in the same public subnet \nNetwork Configuration [Head node in a public subnet and compute fleet in a private subnet]: \nBeginning VPC creation. Please do not leave the terminal until the creation is finalized\nCreating CloudFormation stack...\nDo not leave the terminal until the process has finished.\n</code></pre> <p>If there is an error regarding a failed authorization, there may have been an issue in setting up your policies, make sure you have created the 3 policies correctly.</p>"},{"location":"AWS/getting-started-AWS/#final-cluster-configurations","title":"Final Cluster Configurations","text":"<p>Opening cluster-config.yaml, add the line <code>CustomAmi: &lt;ParaTools-Pro-ami-id&gt;</code> under the Image section. Replacing  with the AMI you obtained in the prior section. <pre><code>Image:\n      Os: ubuntu2204\n      CustomAmi: &lt;ParaTools-Pro-ami-id&gt;\n</code></pre> Furthermore, if you want to be able to RDP/DCV into the head node, then add the \"DCV enabled\" section as shown: <pre><code>HeadNode:\n  Dcv:\n    Enabled: true\n</code></pre>"},{"location":"AWS/getting-started-AWS/#spinning-up-the-cluster-head-node","title":"Spinning up the cluster head node","text":"<p>Now that all configuration is complete, <pre><code>pcluster create-cluster -c cluster.yaml -n name_of_cluster\n</code></pre> This process will return some JSON such as <pre><code>{\n  \"cluster\": {\n    \"clusterName\": \"name_of_cluster\",\n    \"cloudformationStackStatus\": \"CREATE_IN_PROGRESS\",\n    \"cloudformationStackArn\": \"arn:aws:cloudformation:us-west-2:123456789100:stack/name_of_cluster\",\n    \"region\": \"us-west-2\",\n    \"version\": \"3.5.1\",\n    \"clusterStatus\": \"CREATE_IN_PROGRESS\",\n    \"scheduler\": {\n      \"type\": \"slurm\"\n    }\n  },\n  \"validationMessages\": [\n    {\n      \"level\": \"WARNING\",\n      \"type\": \"CustomAmiTagValidator\",\n      \"message\": \"The custom AMI may not have been created by pcluster. You can ignore this warning if the AMI is shared or copied from another pcluster AMI. If the AMI is indeed not created by pcluster, cluster creation will fail. If the cluster creation fails, please go to https://docs.aws.amazon.com/parallelcluster/latest/ug/troubleshooting.html#troubleshooting-stack-creation-failures for troubleshooting.\"\n    },\n    {\n      \"level\": \"WARNING\",\n      \"type\": \"AmiOsCompatibleValidator\",\n      \"message\": \"Could not check node AMI ami-12345678910 OS and cluster OS ubuntu2204 compatibility, please make sure they are compatible before cluster creation and update operations.\"\n    }\n  ]\n}\n</code></pre> This process will take a few minutes to finish. View the progress by performing <code>pcluster list-clusters</code>. If it says creation has failed, a common issue is your pcluster version mismatching the one that created the AMI. Make sure you installed the correct version.</p>"},{"location":"AWS/getting-started-AWS/#accessing-your-cluster","title":"Accessing your cluster","text":"<p>Once your cluster is finished launching, enter the EC2 page, and select Instances. Then select the newly created node, which should be labeled \"Head Node\". In the upper right select Connect and select your method of connection. Note for ssh, the username is likely to be \"ubuntu\", if not, then try to ssh using a conventional terminal, and it should respond with what the username is.</p> <p>Alternatively you can access your cluster from your local console by doing <code>pcluster ssh -i /path/to/key/file -n name_of_cluster</code></p> <p>From there you should be able to launch jobs using slurm.</p>"},{"location":"AWS/getting-started-AWS/#runing-examples","title":"Runing Examples","text":"<p>There is an <code>examples</code> directory with different tests and examples that you can run. For using NVIDIA NeMo\u2122 please see <code>examples/nemo/ex2/text_classification/ex2.sbatch</code>.</p>"},{"location":"GCP/Cluster-Deletion/","title":"Cluster Deletion","text":""},{"location":"GCP/Cluster-Deletion/#proper-deletion","title":"Proper Deletion","text":"<p>It is very important that when you are done using the cluster you must use ghcp to destroy it. When a cluster is created, ghcp creates resources and adds project metadata tags, if improperly deleted, some of these will remain and you will be charged for them. To delete your cluster correctly, find the instructions in the folder created by ghpc, <code>CLUSTER-IMAGE/instructions.txt</code> and do  <pre><code>./ghpc destroy CLUSTER-IMAGE/\n</code></pre></p>"},{"location":"GCP/Cluster-Deletion/#improper-deletion","title":"Improper Deletion","text":""},{"location":"GCP/Cluster-Deletion/#case-1","title":"Case 1","text":"<p>When the compute instances are deleted, but not the folder, you can run the command <code>./ghpc destroy CLUSTER-IMAGE/</code> and it should properly remove all the created resources. You should also run <code>rm -rf CLUSTER-IMAGE/</code> to remove the file.</p>"},{"location":"GCP/Cluster-Deletion/#case-2","title":"Case 2","text":"<p>When the folder hasn't been deleted, and you attempt to create the cluster again, you may get the error <pre><code>  Error: Failed to overwrite existing deployment.\n\n        Use the -w command line argument to enable overwrite.\n        If overwrite is already enabled then this may be because you are attempting to remove a deployment group, which is not supported.\n    original error: the directory already exists: ppro-4-e4s-23-11-cluster-slurm-rocky8\n</code></pre> In this case remove the folder as stated above.</p>"},{"location":"GCP/Cluster-Deletion/#case-3","title":"Case 3","text":"<p>If you are getting the below errors, it indicates ghpc is unable to recreate a cluster due to leftover resources.  <pre><code>Error: Error creating Address: googleapi: Error 409: The resource 'projects/YOUR-PROJECT/regions/us-central1/addresses/CLUSTER-IMAGE' already exists, alreadyExists\n\nwith module.network1.module.nat_ip_addresses[\"us-central1\"].google_compute_address.ip[1],\non .terraform/modules/network1.nat_ip_addresses/main.tf line 50, in resource \"google_compute_address\" \"ip\":\n  50: resource \"google_compute_address\" \"ip\" {\n</code></pre> And also errors like  <pre><code>Error: key \"e4s2311clu-slurm-compute-script-ghpc_startup_sh\" already present in metadata for project \"e4s-pro\". Use `terraform import` to manage it with Terraform\n\n with module.slurm_controller.module.slurm_controller_instance.google_compute_project_metadata_item.compute_startup_scripts[\"ghpc_startup_sh\"],\n  on .terraform/modules/slurm_controller.slurm_controller_instance/terraform/slurm_cluster/modules/slurm_controller_instance/main.tf line 281, in resource \"google_compute_project_metadata_item\" \"compute_startup_scripts\":\n 281: resource \"google_compute_project_metadata_item\" \"compute_startup_scripts\" {\n</code></pre> You must now go through the process of manually deleting each of the keys that are listed in the error list.  As shown here, we will use <code>gcloud compute project-info describe</code> to see the cloud metadata, and <code>gcloud compute project-info remove-metadata --keys=\"the key\" --project=YOUR-PROJECT</code>. You can either run this command once using a list, such as  <pre><code>gcloud compute project-info remove-metadata --keys==[\"CLUSTER-IMAGEclu-slurm-compute-script-ghpc_startup_sh\",\"CLUSTER-IMAGEclu-slurm-controller-script-ghpc_startup_sh\", \u2026 ]\n</code></pre> where you put in each relevant key. Be very careful in this process that you only delete the relevant keys as this metadata info can affect all of you projects. Or you can also do it one at a time, <pre><code>gcloud compute project-info remove-metadata --keys=\"CLUSTER-IMAgE-clu-slurm-controller-script-ghpc_startup_sh\" for each  key listed in the error message.\n</code></pre> In my case the command looked like: <pre><code>gcloud compute project-info remove-metadata --keys=[\"e4s2311clu-slurm-compute-script-ghpc_startup_sh\",\"e4s2311clu-slurm-controller-script-ghpc_startup_sh\",\"e4s2311clu-slurm-tpl-slurmdbd-conf\",\"e4s2311clu-slurm-tpl-cgroup-conf\",\"e4s2311clu-slurm-tpl-slurm-conf\",\"e4s2311clu-slurm-partition-compute-script-ghpc_startup_sh\",\"e4s2311clu-slurm-compute-script-ghpc_startup_sh\",\"e4s2311clu-slurm-controller-script-ghpc_startup_sh\",\"e4s2311clu-slurm-tpl-slurmdbd-conf\",\"e4s2311clu-slurm-tpl-cgroup-conf\"]\n</code></pre> Furthermore, the networking, and filestore resources will still be active, so those must be deleted. By searching filestore you should the instances page, in my case it looks like this   I know that this is the filestore created by the instance I improperly deleted. In your case you must be 100% sure, because if you delete the wrong one you will delete data for other clusters. Be sure to check the creation date and delete.  By searching in your project you should be able to find the network resource page,   You must delete all resources that are listed in the <code>Error 409: The resource 'projects/YOUR-PROJECT/regions/us-central1/addresses/CLUSTER-IMAGE' already exists</code> errors. For network resources they often have to be deleted in a specfic order. It is likely that you should delete the NAT gateway, then the subnetwork, and then the VPC network peering, router, and then VPC, then release the IP address. If you can't delete a resource, it is in use by another. Find and delete the prerequisite resources first, then delete it. Now you should run <code>./ghpc create CLUSTER-IMAGE/</code> If any stray resources still exist, delete them as shown above and rerun these two commands.</p>"},{"location":"GCP/blueprint/","title":"SLURM Scheduler Cluster Blueprint for GCP","text":""},{"location":"GCP/blueprint/#general-info","title":"General Info","text":"<p>Below is an example Google HPC-Toolkit bluiprint for using ParaTools Pro for E4S\u2122. Once you have access to ParaTools Pro for E4S\u2122 through the GCP marketplace, we recommend following the \"quickstart tutorial\" from the Google HPC-Toolkit project to get started if you are new to GCP and/or HPC-Toolkit. The ParaTools Pro for E4S\u2122 blueprint provided below can be copied with some small modifications and used for the tutorial or in production.</p> <p>Areas of the blueprint that require your attention and that may need to be changed are highlighted and have expandable annotations offering further guidance.</p>"},{"location":"GCP/blueprint/#paratools-pro-for-e4stm-slurm-cluster-blueprint-example","title":"ParaTools Pro for E4S\u2122 Slurm Cluster Blueprint Example","text":"e4s-23.11-cluster-slurm-gcp-5-9-hpc-rocky-linux-8.yaml<pre><code>blueprint_name: ppro-4-e4s-23-11-cluster-slurm-gcp-5-9-hpc-rocky-linux-8\n\nvars:\n#  project_id:  \"paratools-pro\" #(1)\n  deployment_name: ppro-4-e4s-23-11-cluster-slurm-rocky8\n  image_family: ppro-4-e4s-23-11-slurm-gcp-5-9-hpc-rocky-linux-8 #(2)!\n  region: us-central1\n  zone: us-central1-a\n  disk_size_gb: 130\n\ndeployment_groups:\n- group: primary\n  modules:\n  - id: network1\n    source: modules/network/vpc\n    settings:\n      firewall_rules:\n        - name: ssh-login\n          direction: INGRESS\n          ranges: [0.0.0.0/0] #(3)!\n          allow:\n            - protocol: tcp\n              ports: [22]\n\n  - id: homefs\n    source: modules/file-system/filestore\n    use: [network1]\n    settings:\n      local_mount: /home\n\n  - id: compute_node_group\n    source: community/modules/compute/schedmd-slurm-gcp-v5-node-group\n    settings:\n      node_count_dynamic_max: 2 #(4)!\n      machine_type: c3-standard-88\n      disk_type: pd-balanced\n      enable_smt: false\n      disk_size_gb: $(vars.disk_size_gb)\n      instance_image:\n        family: $(vars.image_family)\n        project: $(vars.project_id)\n      instance_image_custom: true\n      bandwidth_tier: gvnic_enabled #(6)!\n\n  - id: compute_partition\n    source: community/modules/compute/schedmd-slurm-gcp-v5-partition\n    use:\n    - network1\n    - homefs\n    - compute_node_group\n    settings:\n      partition_name: compute\n      is_default: true\n      enable_placement: true\n\n  - id: h3_node_group\n    source: community/modules/compute/schedmd-slurm-gcp-v5-node-group\n    settings:\n      node_count_dynamic_max: 2\n      machine_type: h3-standard-88\n      disk_type: pd-balanced\n      instance_image:\n        family: $(vars.image_family)\n        project: $(vars.project_id)\n      instance_image_custom: true\n      bandwidth_tier: gvnic_enabled\n\n  - id: h3_partition #(5)!\n    source: community/modules/compute/schedmd-slurm-gcp-v5-partition\n    use:\n    - network1\n    - homefs\n    - h3_node_group\n    settings:\n      partition_name: h3\n\n  - id: slurm_controller\n    source: community/modules/scheduler/schedmd-slurm-gcp-v5-controller\n    use:\n    - network1\n    - compute_partition\n    - h3_partition\n    - homefs\n    settings:\n      disable_controller_public_ips: false\n      disk_size_gb: $(vars.disk_size_gb)\n      instance_image:\n        family: $(vars.image_family)\n        project: $(vars.project_id)\n      instance_image_custom: true\n\n  - id: slurm_login\n    source: community/modules/scheduler/schedmd-slurm-gcp-v5-login\n    use:\n    - network1\n    - slurm_controller\n    settings:\n      machine_type: n2-standard-4\n      disable_login_public_ips: false\n      disk_size_gb: $(vars.disk_size_gb)\n      instance_image:\n        family: $(vars.image_family)\n        project: $(vars.project_id)\n      instance_image_custom: true\n</code></pre> <ol> <li> <p>Warning</p>    Either uncomment this line and ensure that this matches the name of your project on GCP,    or invoke <code>ghpc</code> with the <code>--vars project_id=\"${PROJECT_ID}\"</code> flag. </li> <li> <p>Info</p>    Ensure that this matches the image family from the GCP marketplace </li> <li> <p>Danger</p> <code>0.0.0.0/0</code> exposes TCP port 22 the entire world, fine for testing ephemeral clusters,    but for persistent clusters you should limit traffic to your organizations IP range or    a hardened bastion server </li> <li> <p>Info</p>    The <code>machine_type</code> and <code>node_count_dynamic_max</code> should be set to reflect the instance    types and number of nodes you would like to use. These are spun up dynamically. You    must ensure that you have sufficient quota to run with the number of vCPUs = (cores    per node)*(node_cound_dynamic_max). For compute intensive, tightly coupled jobs, C3    or H3 instances have shown good performance. </li> <li> <p>Info</p>    This example includes an additional SLURM partition containing H3 nodes. At the time    of this writing, access to H3 instances was limited and you may need to request access    via a quota increase request. You do not need multiple SLURM partitions, and may    consider removing this one. </li> <li> <p>Info</p>    To access the full high-speed per-VM Tier_1 networking capabilities on supported instance types, the    gvnic must be enabled. </li> </ol>"},{"location":"GCP/getting-started-GCP/","title":"ParaTools Pro for E4S\u2122 Getting Started with Google Cloud Platform (GCP)","text":""},{"location":"GCP/getting-started-GCP/#general-background-information","title":"General Background Information","text":"<p>In the following tutorial, we roughly follow the same steps as the \"quickstart tutorial\" from the Google HPC-Toolkit project. For the purposes of this tutorial, we make the following assumptions:</p> <ul> <li>You have created a Google Cloud account.</li> <li>You have created a Google Cloud project appropriate for this tutorial   and it is selected.</li> <li>You have setup billing for your Google Cloud Project.</li> <li>You have enabled the Compute Engine API.</li> <li>You have enabled the Filestore API.</li> <li>You have enabled the Cloud Storage API</li> <li>You have enabled the Sevice Usage API.</li> <li>You have enabled the Secret Manager API.</li> <li>You are aware of the costs for running instances on GCP Compute Engine and   of the costs of using the ParaTools Pro for E4S\u2122 GCP marketplace VM image. </li> <li>You are comfortable using the GCP Cloud Shell, or are running locally     (which will match this tutorial) and are familiar with SSH, a terminal and have     installed and initialized the gcloud CLI</li> </ul>"},{"location":"GCP/getting-started-GCP/#tutorial","title":"Tutorial","text":""},{"location":"GCP/getting-started-GCP/#getting-set-up","title":"Getting Set Up","text":"<p>First, let's grab your <code>PROJECT_ID</code> and <code>PROJECT_NUMBER</code>. Navigate to the GCP project selector and select the project that you'll be using for this tutorial. Take note of the <code>PROJECT_ID</code> and <code>PROJECT_NUMBER</code> Open your local shell or the GCP Cloud Shell, and run the following commands: <pre><code>export PROJECT_ID=&lt;enter your project ID here&gt;\nexport PROJECT_NUMBER=&lt;enter your project number here&gt;\n</code></pre></p> <p>Set a default project you will be using for this tutorial. If you have multiple projects you can switch back to a different one when you are finished.</p> <pre><code>gcloud config set project \"${PROJECT_ID}\"\n</code></pre> <p>Next, ensure that the default Compute Engine service account is enabled: <pre><code>gcloud iam service-accounts enable \\\n     --project=\"${PROJECT_ID}\" \\\n     ${PROJECT_NUMBER}-compute@developer.gserviceaccount.com\n</code></pre> and add the <code>roles/editor</code> IAM role to the service account:</p> <pre><code>gcloud projects add-iam-policy-binding \"${PROJECT_ID}\" \\\n    --member=serviceAccount:${PROJECT_NUMBER}-compute@developer.gserviceaccount.com \\\n    --role=roles/editor\n</code></pre>"},{"location":"GCP/getting-started-GCP/#install-the-google-cloud-hpc-toolkit","title":"Install the Google Cloud HPC-Toolkit","text":"<p>First install the dependencies of <code>ghpc</code>. Instructions to do this are included below. If you encounter trouble please check the latest instructions from Google, available here. If you are running the google cloud shell you do not need to install the dependencies and can skip to cloning the hpctoolkit.</p> <p>Install the Google Cloud HPC-Toolkit  Prerequisites</p> <p>Please download and install any missing software packages from the following list:</p> <ul> <li>Terraform version 1.2.0 or later</li> <li>Packer version 1.7.9 or later</li> <li>Go version 1.188 or later. Ensure that the <code>GOPATH</code> is setup and <code>go</code> is on your <code>PATH</code>.   You may need to add the following to <code>.profile</code> or <code>.bashrc</code> startup \"dot\" file:   <pre><code>export PATH=$PATH:$(go env GOPATH)/bin\n</code></pre></li> <li>Git</li> <li><code>make</code> (see below for instructions specific to your OS)</li> </ul> macOSUbuntu/DebianCentOS/RHEL <p><code>make</code> is packaged with the Xcode command line developer tools on macOS. To install, run: <pre><code>xcode-select --install\n</code></pre></p> <p>Install <code>make</code> with the OS' package manager: <pre><code>apt-get -y install make\n</code></pre></p> <p>Install <code>make</code> with the OS' package manager: <pre><code>yum install -y make\n</code></pre></p> <p>Note</p> <p>Most of the packages above may be installable through your OSes package manager. For example, if you have Homebrew on macOS you should be able to <code>brew install &lt;package_name&gt;</code> for most of these items, where <code>&lt;package_name&gt;</code> is, e.g., <code>go</code>.</p> <p>Once all the software listed above has been verified and/or installed, clone the Google Cloud HPC-Toolkit and change directories to the cloned repository: <pre><code>git clone https://github.com/GoogleCloudPlatform/hpc-toolkit.git\ncd hpc-toolkit/\n</code></pre> Next build the HPC-Toolkit and verify the version and that it built correctly. <pre><code>make\n./ghpc --version\n</code></pre> If you would like to install the compiled binary to a location on your <code>$PATH</code>, run <pre><code>sudo make install\n</code></pre> to install the <code>ghpc</code> binary into <code>/usr/local/bin</code>, of if you do not have root priviledges or do not want to install the binary into a system wide location, run <pre><code>make install-user\n</code></pre> to install <code>ghpc</code> into <code>${HOME}/bin</code> and then ensure this is on your path:</p> <pre><code>export PATH=\"${PATH}:${HOME}/bin\"\n</code></pre>"},{"location":"GCP/getting-started-GCP/#grant-adc-access-to-terraform-and-enable-os-login","title":"Grant ADC access to Terraform and Enable OS Login","text":"<p>Generate cloud credentials associated with your Google Cloud account and grant Terraform access to the Aplication Default Credential (ADC).</p> <p>Note</p> <p>If you are using the Cloud Shell you can skip this step.</p> <pre><code>gcloud auth application-default login\n</code></pre> <p>To be able to connect to VMs in the cluster OS Login must be enabled. Unless OS Login is already enabled at the organization level, enable it at the project level. To do this, run:</p> <pre><code>gcloud compute project-info add-metadata \\\n     --metadata enable-oslogin=TRUE\n</code></pre>"},{"location":"GCP/getting-started-GCP/#deploy-the-cluster","title":"Deploy the Cluster","text":"<p>Copy the ParaTools-Pro-slurm-cluster-blueprint-example from the ParaTools Pro for E4S\u2122 documentation to your clipboard, then paste it into a file named <code>ParaTools-Pro-Slurm-Cluster-Blueprint.yaml</code>. After copying the text, in your terminal do the following:</p> <pre><code>cat &gt; ParaTools-Pro-Slurm-Cluster-Blueprint.yaml\n# paste the copied text # (1)\n# press Ctrl-d to add an end-of-file character\ncat ParaTools-Pro-Slurm-Cluster-Blueprint.yaml # Check the file copied correctly #(2)\n</code></pre> <ol> <li> <p>Note</p>    Usually <code>Ctrl-v</code>, or <code>Command-v</code> on macOS </li> <li> <p>Note</p>    This is optional, but usually a good idea </li> </ol> <p>Using your favorite editor, select appropriate instance types for the compute partitions, and remove the h3 partition if you do not have access to h3 instances yet. See the expandable annotations and pay extra attention to the highlighted lines on the ParaTools-Pro-slurm-cluster-blueprint-example example.</p> <p>Pay Attention</p> <p>In particular:</p> <ul> <li>Determine if you want to pass the <code>${PROJECT_ID}</code> on the command line or in the blueprint</li> <li>Verify that the <code>image_family</code> key matches the image for ParaTools Pro for E4S\u2122 from the GCP marketplace</li> <li>Adjust the region and zone used, if desired</li> <li>Limit the IP <code>ranges</code> to those you will be connecting from via SSH in the <code>ssh-login</code> <code>firewall_rules</code> rule, if in a production setting.   If you plan to connect only from the cloud shell the <code>ssh-login</code> <code>firewall_rules</code> rule may be completely removed.</li> <li>Set an appropriate <code>machine_type</code> and <code>dynamic_node_count_max</code> for your <code>compute_node_group</code>.</li> </ul> <p>Once the blue print is configured to be consistent with your GCP usage quotas and your preferences, set deployment variables and create the deployment folder.</p> <p>Create deployment folder</p> <pre><code>./ghpc create e4s-23.11-cluster-slurm-gcp-5-9-hpc-rocky-linux-8.yaml \\\n  --vars project_id=${PROJECT_ID} # (1)!\n</code></pre> <ol> <li> <p>Note</p>    If you uncommented and updated the <code>vars.project_id:</code> you do not need to pass    <code>--vars project_id=...</code> on the command line.    If you're bringing a cluster back online that was previously deleted, but    the blueprint has been modified and the deployment folder is still present,    the <code>-w</code> flag will let you overwrite the deployment folder contents with the    latest changes. </li> </ol> Note <p>It may take a few minutes to finish provisioning your cluster.</p> <p>Now the cluster can be deployed. Run the following command to deploy your ParaTools Pro for E4S\u2122 cluster:</p> <p>Perform the deployment</p> <pre><code>./ghpc deploy ppro-4-e4s-23-11-cluster-slurm-rocky8\n</code></pre> <p>At this point you will be prompted to review or accept the proposed changes. You may review them if you like, but you should press <code>a</code> for accept once satisfied.</p>"},{"location":"GCP/getting-started-GCP/#connect-to-the-cluster","title":"Connect to the Cluster","text":"<p>Once the cluster is deployed, ssh to the login node.</p> <ol> <li> <p>Go to the Compute Engine &gt; VM Instances page.</p> <p>GCP VM Instances</p> </li> <li> <p>Click on <code>ssh</code> for the login node of the cluster. You may need to approve Google authentication before the session can connect.</p> </li> </ol>"},{"location":"GCP/getting-started-GCP/#deletion-of-the-cluster","title":"Deletion of the Cluster","text":"<p>It is very important that when you are done using the cluster you must use ghcp to destroy it. If your instances were deleted in a different manner, see here. To delete your cluster correctly do</p> <p><pre><code>./ghpc destroy ppro-4-e4s-23-11-cluster-slurm-rocky8\n</code></pre> At this point you will be prompted to review or accept the proposed changes. You may review them if you like, but you should press <code>a</code> for accept once satisfied and the deletion will proceed.</p>"},{"location":"ODDC/getting-started-ODDC/","title":"E4S Pro Getting Started with Adaptive On-Demand Data Center (ODDC)","text":""},{"location":"ODDC/getting-started-ODDC/#general-background-information","title":"General Background Information","text":"<p>In this tutorial we will show you how to launch an HPC cluster on ODDC using the ODDC web interface.</p> <p>This tutorial assumes you have access to a functioning ODDC server, installed and configured as described in the ODDC Documentation.</p>"},{"location":"ODDC/getting-started-ODDC/#tutorial","title":"Tutorial","text":""},{"location":"ODDC/getting-started-ODDC/#log-in","title":"Log In","text":"<p>Enter the URL for your ODDC web portal instance in a web browser and log in with the credentials you created during configuration. </p>"},{"location":"ODDC/getting-started-ODDC/#createlaunch-cluster","title":"Create/Launch Cluster","text":"<p>Mouse over the menu on the left and select Cluster Manager.</p> <p>You can create or import a new cluster using the '+' button in the upper right. Once your cluster is configured hit the ellipsis '...' on the right side of your cluster entry and select the deploy option. The portal will display the progress of the cluster initialization and let you know when the cluster is available.</p>"},{"location":"ODDC/getting-started-ODDC/#launch-jobs","title":"Launch Jobs","text":"<p>Once your cluster has been launched you can create and issue jobs from the Job Manager, available via the menu on the right. </p>"},{"location":"ODDC/getting-started-ODDC/#ssh-to-cluster","title":"SSH To Cluster","text":"<p>To log into the cluster you can find the cluster's public IP address at the end of the startup log, available from the cluster's ellipsis menu while the cluster is active. </p> <p>Alternatively you can ssh to the ODDC server and use the ODDC CLI to log in. To do this you must first configure a VPN to access the ODDC server. We recommend WireGuard. Once WireGuard is installed and configured with your authentication credentials and the address of the ODDC server you can ssh to the server. From the ODDC server you can log in to your cluster like <code>oddc cluster:ssh &lt;cluster-name&gt;</code> Add the <code>--admin</code> flag for access to sudo privileges on the cluster.</p>"},{"location":"ODDC/getting-started-ODDC/#cleanup","title":"Cleanup","text":"<p>When you are finished de-allocate the cluster by entering its ellipsis menu and selecting destroy. The portal will show the destruction's progress and indicate when it is complete. Once the cluster has been deactivate it may be deleted entirely via the delete option in the ellipsis menu or kept in your cluster list for a new instance to be initialized later.</p>"}]}